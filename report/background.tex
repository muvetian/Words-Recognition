
\section{Background}
\label{sec:background}
Our first observation into the data displayed a large number of unused columns
and unused features. The grey-scale consistently showed a zero and was unused in
the analyzation. We therefore used a backward elimination method to remove many
of the unused features.\\

We investigated into the course of multi-layer perceptron. This is a
Neural Network regression classifier that includes a hidden layer
where the input is transformed into a more separable layer. Many
hidden layers can be added to the model and we used two different
transformations to create the hidden data. The first is the identity
which is a no-operation activation. It just returns the same out put
as was inputed or $f(x)=x$. The second transformation we used was
described in the paper “One Against One” or “One Against All”: Which One is
Better for Handwriting Recognition with SVMs?,
and transformed the input into a rectified linear unit function.\cite{milgram} This
transformation returns $f(x)=max(0,x)$ and was used in the paper as a
baseline but we had not fully understood their further studies and so
this became one of our better classifiers. \\

Looking more into the multi-layer transformations we came across
Reading Checks with Multilayer Graph Transformer Networks
\cite{cun}. This paper went even more in-depth into multilayer neural
net training. It spoke a lot about its use in efficiently computing
the gradients of the function as well as going into the mathematical
side of the hidden layers or as they called it, the convolutional
layer. We used some of their research and testing in Gradient Based
Learning to determine the number of layers to use as well as which
classifiers not to use. \\
