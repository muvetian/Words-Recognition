
\section{Background}
\label{sec:background}

We investigated into the course of multi-layer perceptron. This is a
Neural Network regression classifier that includes a hidden layer
where the input is transformed into a more seperable layer. Many
hidden layers can be added to the model and we used two different
transformations to create the hidden data. The first is the identity
which is a no-operation activation. It just returns the same out put
as was inputed or $f(x)=x$. The second transformation we used was
described in the paper tilte,
and transformed the input into a rectified linear unit function.\cite{milgram} This
transformation returns $f(x)=max(0,x)$ and was used in the paper as a
baseline but we had not fully understood their further studys and so
this became one of our better classifiers. \\

Looking more into the multi-layer transformations we came across
Reading Checks with Multilayer Graph Transformer Netwroks
\cite{cun}. This paper went even more indepth into multilayer neural
net training. It spoke a lot about its use in efficiently computing
the gradients of the function as well as going into the mathematical
side of the hidden layers or as they called it, the convolutional
layer. We used some of their research and testing in Gradient Based
Learning to determine the number of layers to use as well as which
classifiers not to use. \\




