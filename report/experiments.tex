
\section{Experiments}
\label{sec:expts}
It should be noted that in the original data set if every pixel grey scale value
 were to be used as a feature to train for the model might result in a huge set
 of features. Having such set of features may cause a common phenomena known as
 "the curse of dimensionality", which implicates that the increase in the
 dimensional space resulted from the increase in the number of features might
 dilute the statistical significance of the final result.\cite{bellman}
That being said, in order to reduce the complexity of the final model in the
hope of avoiding overfitting problems, a feature selection process was
implemented for the experiments.\cite{hall}

\subsection{Feature Selections}
\label{feature}
According to Hall, feature selection process consists of the following steps:
\begin{itemize}
	\item Starting point
	\item Search organization
	\item Evaluation strategy
	\item Stopping criterion

\end{itemize}
As introduced in the "Background" section, after some inspections through the
data sets, it was observed that some pixels' grey scale values stayed the same
all the time. Those points generally tend to have no significant impacts on the
predictions according to the low-variance rule. Thus, those features could be
the starting points. For this purpose, the backward elimination method was
adopted to reduce the number of features.\cite{hall}
The evaluation strategy is basically reducing the features that had the lowest
chi-squared scores one by one using the chi-squared feature selection method
implemented in the scikit-learn library. This process would be repeated until
the results showed significant decline in the performance of the models
generated by the learning algorithms. For the purpose of determining the
optimal set of features, the SVM algorithm was implemented for its short
running time.

\subsection{Classifiers}
\label{class}
For this particular classification problem, several models were implemented and
tested out.
\paragraph{Super Vector Machines}
SVM serves as the baseline model for the experiments due to its simplistic
nature. It is also used for finding the optimal set of features.
\paragraph{Multi-layered Perceptron}
We used two different versions of MLP in our experimentation. The first used a
linear transformation to interact between the input and the hidden layer while
the other transformed the input using a rectified linear unit function. We
constructed both because each gave us a very different outcome and it was shown
in our data below. These were discovered to be much better classifiers than our
 baseline as the neural networking gave them an advantage over the linear
 modeling.
